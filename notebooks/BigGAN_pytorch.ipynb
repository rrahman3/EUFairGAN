{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', '', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "print(sys.path)\n",
    "\n",
    "from src.pretrained.pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
    "                                       save_as_images, display_in_terminal)\n",
    "\n",
    "from src.pretrained.BigGAN_PyTorch import BigGANdeep\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Import Discriminator\n",
    "from src.pretrained.BigGAN_PyTorch.BigGANdeep import Discriminator, Generator\n",
    "\n",
    "D = Discriminator(n_classes=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count for Gs initialized parameters: 17279555\n"
     ]
    }
   ],
   "source": [
    "G = Generator(n_classes=14, G_param='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Distribution([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.pretrained.BigGAN_PyTorch.utils import Distribution\n",
    "device = 'cpu'\n",
    "y_ = Distribution(torch.zeros(32, requires_grad=False))\n",
    "y_.init_distribution('categorical',num_categories=14)\n",
    "y_ = y_.to(device, torch.int64)\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "torch.Size([32])\n",
      "Distribution([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1, 256])\n",
      "in forward z = torch.Size([32, 256]), y = torch.Size([32, 1, 256])\n",
      "torch.Size([32, 16384])\n",
      "torch.Size([32, 1024, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from src.pretrained.BigGAN_PyTorch.utils import prepare_z_y\n",
    "z_, y_ = prepare_z_y(G_batch_size=32, dim_z=256, nclasses=14, device='cpu')\n",
    "print(z_.shape)\n",
    "print(y_.shape)\n",
    "print(y_)\n",
    "y_ = y_.unsqueeze(-1).float()\n",
    "print(y_.shape)\n",
    "# torch.nn.Embedding()\n",
    "y_ = torch.randint(0, 2, (32, 1, 256)).float()\n",
    "print(y_.shape)\n",
    "output = G(z_, y_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "torch.Size([3, 128, 128])\n",
      "(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(output[0].shape)\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "# Assuming output is a PyTorch tensor\n",
    "# output = output.squeeze(0)  # Remove batch dimension if present\n",
    "for i in range (0, output.shape[0]):\n",
    "    print(output[i].shape)\n",
    "    output_numpy = output[i].detach().permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Convert the NumPy array to an image (ensure the values are in [0, 255] for uint8 type)\n",
    "    # output_image = Image.fromarray((output_numpy * 255).astype('uint8'))\n",
    "    # obj = obj.transpose((0, 2, 3, 1))\n",
    "    output_numpy = np.clip(((output_numpy + 1) / 2.0) * 256, 0, 255)\n",
    "\n",
    "    # Show the image\n",
    "    print(output_numpy.shape)\n",
    "    output_numpy = np.asarray(np.uint8(output_numpy), dtype=np.uint8)\n",
    "    output_image = Image.fromarray(output_numpy, mode=\"RGB\")\n",
    "    output_image.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(z_.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-7.5374, -4.0496, -4.0196,  ..., -3.9852, -4.1534, -5.7108],\n",
       "          [-7.4978, -4.0239, -3.9911,  ..., -3.9579, -4.1240, -5.6854],\n",
       "          [-7.5285, -4.0496, -4.0153,  ..., -3.9817, -4.1505, -5.7147],\n",
       "          ...,\n",
       "          [-7.5531, -4.0599, -4.0293,  ..., -3.9948, -4.1641, -5.7205],\n",
       "          [-7.6465, -4.1297, -4.1009,  ..., -4.0639, -4.2351, -5.7710],\n",
       "          [-7.5938, -4.0953, -4.0609,  ..., -4.0271, -4.2017, -5.7637]]],\n",
       "\n",
       "\n",
       "        [[[-8.0542, -4.0663, -3.8793,  ..., -3.9852, -4.0117, -6.4737],\n",
       "          [-8.0129, -4.0409, -3.8558,  ..., -3.9579, -3.9840, -6.4492],\n",
       "          [-8.0445, -4.0672, -3.8781,  ..., -3.9817, -4.0082, -6.4809],\n",
       "          ...,\n",
       "          [-8.0708, -4.0767, -3.8886,  ..., -3.9948, -4.0214, -6.4834],\n",
       "          [-8.1678, -4.1467, -3.9497,  ..., -4.0639, -4.0908, -6.5258],\n",
       "          [-8.1127, -4.1129, -3.9229,  ..., -4.0271, -4.0545, -6.5315]]],\n",
       "\n",
       "\n",
       "        [[[-7.4082, -4.0455, -4.0547,  ..., -3.9852, -4.1888, -5.5200],\n",
       "          [-7.3691, -4.0196, -4.0249,  ..., -3.9579, -4.1590, -5.4945],\n",
       "          [-7.3995, -4.0452, -4.0496,  ..., -3.9817, -4.1861, -5.5231],\n",
       "          ...,\n",
       "          [-7.4237, -4.0557, -4.0645,  ..., -3.9948, -4.1998, -5.5297],\n",
       "          [-7.5162, -4.1254, -4.1387,  ..., -4.0639, -4.2712, -5.5823],\n",
       "          [-7.4640, -4.0909, -4.0953,  ..., -4.0271, -4.2385, -5.5718]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-7.6666, -4.0538, -3.9845,  ..., -3.9852, -4.1179, -5.9015],\n",
       "          [-7.6266, -4.0281, -3.9572,  ..., -3.9579, -4.0890, -5.8763],\n",
       "          [-7.6575, -4.0540, -3.9810,  ..., -3.9817, -4.1149, -5.9062],\n",
       "          ...,\n",
       "          [-7.6826, -4.0641, -3.9941,  ..., -3.9948, -4.1284, -5.9112],\n",
       "          [-7.7769, -4.1339, -4.0631,  ..., -4.0639, -4.1991, -5.9597],\n",
       "          [-7.7235, -4.0997, -4.0264,  ..., -4.0271, -4.1649, -5.9557]]],\n",
       "\n",
       "\n",
       "        [[[-7.5374, -4.0496, -4.0196,  ..., -3.9852, -4.1534, -5.7108],\n",
       "          [-7.4978, -4.0239, -3.9911,  ..., -3.9579, -4.1240, -5.6854],\n",
       "          [-7.5285, -4.0496, -4.0153,  ..., -3.9817, -4.1505, -5.7147],\n",
       "          ...,\n",
       "          [-7.5531, -4.0599, -4.0293,  ..., -3.9948, -4.1641, -5.7205],\n",
       "          [-7.6465, -4.1297, -4.1009,  ..., -4.0639, -4.2351, -5.7710],\n",
       "          [-7.5938, -4.0953, -4.0609,  ..., -4.0271, -4.2017, -5.7637]]],\n",
       "\n",
       "\n",
       "        [[[-7.6666, -4.0538, -3.9845,  ..., -3.9852, -4.1179, -5.9015],\n",
       "          [-7.6266, -4.0281, -3.9572,  ..., -3.9579, -4.0890, -5.8763],\n",
       "          [-7.6575, -4.0540, -3.9810,  ..., -3.9817, -4.1149, -5.9062],\n",
       "          ...,\n",
       "          [-7.6826, -4.0641, -3.9941,  ..., -3.9948, -4.1284, -5.9112],\n",
       "          [-7.7769, -4.1339, -4.0631,  ..., -4.0639, -4.1991, -5.9597],\n",
       "          [-7.7235, -4.0997, -4.0264,  ..., -4.0271, -4.1649, -5.9557]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "n_classes = 14\n",
    "images = torch.rand(batch_size, 3, 128, 128)\n",
    "labels = torch.randint(0, 2, (batch_size, n_classes, 1))\n",
    "D(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 3  # Number of channels (e.g., 3 for RGB images, 1 for grayscale)\n",
    "image_size = 64  #\n",
    "\n",
    "batch_size = 16  # Number of images in the batch\n",
    "random_image_input = torch.randn(batch_size, input_channels, image_size, image_size)  # [B, C, H, W]\n",
    "class_vector_dim = 14\n",
    "class_vectors = torch.randint(0, 2, (batch_size, class_vector_dim, 1), dtype=torch.long)  # Random class vector\n",
    "print(class_vectors.shape)\n",
    "# print(class_vectors)\n",
    "print(random_image_input.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the discriminator\n",
    "output = D(random_image_input, class_vectors)\n",
    "\n",
    "# Print output shape and values\n",
    "print(f\"Input Shape: {random_image_input.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Output Values: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "model = BigGAN.from_pretrained('biggan-deep-128')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_classes = 14\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings = torch.nn.Linear(in_features=14, out_features=128, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding\n",
    "\n",
    "# Define custom class embedding for medical conditions\n",
    "num_conditions = 20  # e.g., 20 medical conditions\n",
    "embedding_dim = model.config.class_embed_dim\n",
    "model.class_embed = Embedding(num_conditions, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # if param.requires_grad:\n",
    "    print(f\"Layer: {name:<50} Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Prepare a input\n",
    "truncation = 0.6\n",
    "batch_size = 1\n",
    "# class_vector = one_hot_from_names(['soap bubble', 'coffee', 'mushroom'], batch_size=3)\n",
    "# class_vector = one_hot_from_names(['bee'], batch_size=batch_size)\n",
    "class_vector = np.array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
    "noise_vector = truncated_noise_sample(truncation=truncation, batch_size=batch_size)\n",
    "print(class_vector.shape, noise_vector.shape)\n",
    "# All in tensors\n",
    "noise_vector = torch.from_numpy(noise_vector).float()\n",
    "class_vector = torch.from_numpy(class_vector).float()\n",
    "\n",
    "# Generate an image\n",
    "with torch.no_grad():\n",
    "    output = model(noise_vector, class_vector, truncation)\n",
    "print(output.shape)\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "# Assuming output is a PyTorch tensor\n",
    "# output = output.squeeze(0)  # Remove batch dimension if present\n",
    "for i in range (0, output.shape[0]):\n",
    "    print(output[i].shape)\n",
    "    output_numpy = output[i].permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Convert the NumPy array to an image (ensure the values are in [0, 255] for uint8 type)\n",
    "    # output_image = Image.fromarray((output_numpy * 255).astype('uint8'))\n",
    "    # obj = obj.transpose((0, 2, 3, 1))\n",
    "    output_numpy = np.clip(((output_numpy + 1) / 2.0) * 256, 0, 255)\n",
    "\n",
    "    # Show the image\n",
    "    print(output_numpy.shape)\n",
    "    output_numpy = np.asarray(np.uint8(output_numpy), dtype=np.uint8)\n",
    "    output_image = Image.fromarray(output_numpy, mode=\"RGB\")\n",
    "    output_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All in tensors\n",
    "noise_vector = torch.from_numpy(noise_vector)\n",
    "class_vector = torch.from_numpy(class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an image\n",
    "with torch.no_grad():\n",
    "    output = model(noise_vector, class_vector, truncation)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "# Assuming output is a PyTorch tensor\n",
    "# output = output.squeeze(0)  # Remove batch dimension if present\n",
    "for i in range (0, output.shape[0]):\n",
    "    print(output[i].shape)\n",
    "    output_numpy = output[i].permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Convert the NumPy array to an image (ensure the values are in [0, 255] for uint8 type)\n",
    "    # output_image = Image.fromarray((output_numpy * 255).astype('uint8'))\n",
    "    # obj = obj.transpose((0, 2, 3, 1))\n",
    "    output_numpy = np.clip(((output_numpy + 1) / 2.0) * 256, 0, 255)\n",
    "\n",
    "    # Show the image\n",
    "    print(output_numpy.shape)\n",
    "    output_numpy = np.asarray(np.uint8(output_numpy), dtype=np.uint8)\n",
    "    output_image = Image.fromarray(output_numpy, mode=\"RGB\")\n",
    "    output_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU, put everything on cuda\n",
    "noise_vector = noise_vector.to('cuda')\n",
    "class_vector = class_vector.to('cuda')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU put back on CPU\n",
    "output = output.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as png images\n",
    "save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size=128, num_classes=14, embed_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Feature extractor for images\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_real_fake = nn.Linear(512 * (image_size // 16) ** 2, 1)  # Real vs fake\n",
    "        self.fc_class_embed = nn.Linear(embed_dim, 512 * (image_size // 16) ** 2)\n",
    "\n",
    "        # Class embedding\n",
    "        self.class_embedding = nn.Embedding(num_classes, embed_dim)\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        # Extract image features\n",
    "        features = self.conv_layers(images)  # Shape: [batch_size, 512, image_size//16, image_size//16]\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [batch_size, 512 * (image_size // 16)^2]\n",
    "\n",
    "        # Embed labels\n",
    "        # labels = labels.long()  # Ensure labels are integers\n",
    "        # label_embed = self.class_embedding(labels)  # Shape: [batch_size, embed_dim]\n",
    "        # label_embed = self.fc_class_embed(label_embed)  # Shape: [batch_size, 512 * (image_size // 16)^2]\n",
    "        print(features.shape)\n",
    "            #   , label_embed.shape)\n",
    "        # Combine features and label embeddings\n",
    "        combined = features \n",
    "        # + label_embed  # Element-wise addition\n",
    "\n",
    "        # Output real/fake prediction\n",
    "        output = self.fc_real_fake(combined)  # Shape: [batch_size, 1]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpretrained\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_pretrained_biggan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BigGAN, one_hot_from_names, truncated_noise_sample,\n\u001b[0;32m      8\u001b[0m                                        save_as_images, display_in_terminal)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpretrained\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBigGAN_PyTorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBigGANdeep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Discriminator\n\u001b[0;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pretrained'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from pretrained.pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
    "                                       save_as_images, display_in_terminal)\n",
    "\n",
    "from pretrained.BigGAN_PyTorch.BigGANdeep import Discriminator\n",
    "\n",
    "device = \"cpu\"\n",
    "discriminator = Discriminator(n_classes=14)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "from pretrained.BigGAN_PyTorch.BigGANdeep import Generator\n",
    "# biggan_model = Generator(n_classes=14)\n",
    "biggan_model = BigGAN.from_pretrained('biggan-deep-128')\n",
    "biggan_model.config.num_classes = 14\n",
    "biggan_model.embeddings = torch.nn.Linear(in_features=14, out_features=128, bias=True)\n",
    "biggan_model = biggan_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpretrained\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBigGAN_PyTorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBigGANdeep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Discriminator\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pretrained'"
     ]
    }
   ],
   "source": [
    "from pretrained.BigGAN_PyTorch.BigGANdeep import Discriminator\n",
    "\n",
    "device = \"cpu\"\n",
    "discriminator = Discriminator(n_classes=14)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size=128, num_classes=14, embed_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Feature extractor for images\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_real_fake = nn.Linear(512 * (image_size // 16) ** 2, 1)  # Real vs fake\n",
    "        self.fc_class_embed = nn.Linear(embed_dim, 512 * (image_size // 16) ** 2)\n",
    "\n",
    "        # Class embedding\n",
    "        self.class_embedding = nn.Embedding(num_classes, embed_dim)\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        # Extract image features\n",
    "        features = self.conv_layers(images)  # Shape: [batch_size, 512, image_size//16, image_size//16]\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [batch_size, 512 * (image_size // 16)^2]\n",
    "\n",
    "        # Embed labels\n",
    "        # labels = labels.long()  # Ensure labels are integers\n",
    "        # label_embed = self.class_embedding(labels)  # Shape: [batch_size, embed_dim]\n",
    "        # label_embed = self.fc_class_embed(label_embed)  # Shape: [batch_size, 512 * (image_size // 16)^2]\n",
    "        print(features.shape)\n",
    "            #   , label_embed.shape)\n",
    "        # Combine features and label embeddings\n",
    "        combined = features \n",
    "        # + label_embed  # Element-wise addition\n",
    "\n",
    "        # Output real/fake prediction\n",
    "        output = self.fc_real_fake(combined)  # Shape: [batch_size, 1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN\\\\configs', 'c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN\\\\configs', 'c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN', 'c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN\\\\notebooks', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, os.path.abspath('../configs'))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', '', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\rrahman3\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\rrahman3\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rrahman3\\\\PhDDissertation\\\\EUFairGAN\\\\notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config_reader import ConfigReader\n",
    "configs = ConfigReader(base_path='../configs').load_all_configs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ConfigReader(base_path='configs').load_all_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------Loading NIHCC Chest Xray dataset------------------------------------------\n",
      "Index(['Unnamed: 0', 'id', 'Atelectasis', 'Cardiomegaly', 'Consolidation',\n",
      "       'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration',\n",
      "       'Mass', 'Nodule', 'Pleural Thickening', 'Pneumonia', 'Pneumothorax',\n",
      "       'Pneumoperitoneum', 'Pneumomediastinum', 'Subcutaneous Emphysema',\n",
      "       'Tortuous Aorta', 'Calcification of the Aorta', 'No Finding', 'subj_id',\n",
      "       'Image Index', 'Finding Labels', 'Follow-up #', 'Patient ID',\n",
      "       'Patient Age', 'Patient Gender', 'View Position', 'OriginalImage[Width',\n",
      "       'Height]', 'OriginalImagePixelSpacing[x', 'y]'],\n",
      "      dtype='object')\n",
      "39253\n",
      "Total samples in this dataloader: 39253\n"
     ]
    }
   ],
   "source": [
    "from src.dataloader.dataloader_factory import dataloader_factory\n",
    "from src.utils.config_reader import ConfigReader\n",
    "configs = ConfigReader(base_path='../configs').load_all_configs()\n",
    "dataset_name = \"NIHChestXray\"\n",
    "datasets_config = configs['datasets']\n",
    "dataset_info = datasets_config['datasets'].get(dataset_name)\n",
    "train_loader = dataloader_factory(dataset_name, 'train', dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
